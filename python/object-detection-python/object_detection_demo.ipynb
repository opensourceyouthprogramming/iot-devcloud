{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Detection Demo: Car Detection\n",
    "\n",
    "This is a sample reference implementation to showcase object detection (car in this case) with single-shot detection (SSD) and Async API.\n",
    "Async API improves the overall frame-rate of the application by not waiting for the inference to complete but continuing to do things on the host while inference accelerator is busy. \n",
    "Specifically, this code demonstrates two parallel inference requests by processing the current frame while the next input frame is being captured. This essentially hides the latency of frame capture.\n",
    "\n",
    "## Overview of How it works?\n",
    "At start-up the sample application reads the equivalent of command line arguments and loads a network and image from the video input to the Inference Engine (IE) plugin. \n",
    "A job is submitted to the hardware accelerator (Intel® Core CPU, Intel® HD Graphics GPU, Intel® Core CPU, Intel® Movidius™ and/or Neural Compute Stick)\n",
    "After the inference is completed, the output videos are appropriately stored in the /results directory which can then be viewed within the Jupyter Notebook instance.\n",
    "\n",
    "## Demonstration objectives\n",
    "* Video as input is supported using **OpenCV**\n",
    "* Inference performed on edge hardware (rather than on the development node hosting this Jupyter notebook)\n",
    "* **OpenCV** provides the bounding boxes, labels and other information\n",
    "* Visualization of the resulting bounding boxes\n",
    "* Demonstrate the Async API in action\n",
    "\n",
    "\n",
    "## Step 0: Set Up\n",
    "\n",
    "### 0.1: Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2  (Optional-step): Original video without inference\n",
    "\n",
    "If you are curious to see the input video, run the following cell to view the orignal video stream used for inference and object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ln -sf /data/reference-sample-data/object-detection-python/cars_1900.mp4 \n",
    "videoHTML('Cars video', ['cars_1900.mp4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Using OpenVINO\n",
    "\n",
    "First, let's try running inference on a single image to see how OpenVINO works.\n",
    "We will be using OpenVINO's Inference Engine (IE) to locate vehicles on the road.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create a Intermediate Representation (IR) Model using the Intel Model Optimizer\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### 1.1 Creating IR Model\n",
    "\n",
    "Intel Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existsing DNN models from popular frameworks (e.g. Caffe, TF) using the Intel Model Optimizer. \n",
    "\n",
    "The Intel Distribution of OpenVINO includes a utility script `model_downloader.py` that you can use to download some common modes. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in commannd line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "In this demo, we will be using the **mobilenet-ssd** model. This model can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_downloader/downloader.py --name mobilenet-ssd -o raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "With the `-o` option set as above, this command downloads the model in the directory `raw_models`, with the `.caffemodel` located at `raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel`\n",
    "\n",
    "Now, let's convert this to the optimized model using the model optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP32 \\\n",
    "-o models/mobilenet-ssd/FP32 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the above line is a single command line input, which spans 4 lines thanks to the backslash '\\\\', which is a line continuation character in Bash.\n",
    "\n",
    "Here, the arguments are:\n",
    "* --input-model : the original model\n",
    "* --data_type : Data type to use. One of {FP32, FP16, half, float}\n",
    "* -o : outout dirctory\n",
    "\n",
    "This script also supports `-h` that will you can get the full list of arguments.\n",
    "\n",
    "With the `-o` option set as above, this command will write the output to the directory `models/mobilenet-ssd/FP32`\n",
    "\n",
    "There are two files produced:\n",
    "```\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.xml\n",
    "models/mobilenet-ssd/FP32/mobilenet-ssd.bin\n",
    "```\n",
    "These will be used later in the exercise.\n",
    "\n",
    "We will also be needing the FP16 version of the model for the calculations on the MYRIAD architecture. Run the following cell to create it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/computer_vision_sdk/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model raw_models/object_detection/common/mobilenet-ssd/caffe/mobilenet-ssd.caffemodel \\\n",
    "--data_type FP16 \\\n",
    "-o models/mobilenet-ssd/FP16 \\\n",
    "--scale 256 \\\n",
    "--mean_values [127,127,127] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Choosing Device\n",
    "\n",
    "Now we must select the device used for inferencing. This is done by loading the appropriate plugin to initialize the specified device and load the extensions library (if specified) provided in the extension/ folder for the device.\n",
    "\n",
    "\n",
    "The following cell constructs **`IEPlugin`**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IEPlugin\n",
    "\n",
    "def createPlugin(device, extension_list):\n",
    "    # Plugin initialization for specified device. We will be targeting CPU initially.\n",
    "    plugin = IEPlugin(device=device)\n",
    "\n",
    "    # Loading additional exension libraries for the CPU\n",
    "    for extension in extension_list:\n",
    "        plugin.add_cpu_extension('/data/reference-sample-data/extension/libcpu_extension.so')\n",
    "    \n",
    "    return plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "Currently, three types of plugins are supported: CPU, GPU, and MYRIAD. CPU plugin may require additional extensions to improve performance, abd `add_cpu_extension` function is used to load these additional extensions.\n",
    "\n",
    "\n",
    "### 1.3 Read the IR (Intermediate Representation) model\n",
    "\n",
    "We can import optimized models (weights) from step 1.1 into our neural network using **`IENetwork`**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openvino.inference_engine import IENetwork\n",
    "\n",
    "def createNetwork(model_xml, model_bin, plugin):\n",
    "    # Importing network weights from IR models.\n",
    "    net = IENetwork.from_ir(model=model_xml, weights=model_bin)\n",
    "    \n",
    "    # Some layers in IR models may be unsupported by some plugins. \n",
    "    if \"CPU\" in plugin.device:\n",
    "        supported_layers = plugin.get_supported_layers(net)\n",
    "        not_supported_layers = [l for l in net.layers.keys() if l not in supported_layers]\n",
    "        if len(not_supported_layers) != 0:\n",
    "            print(\"Following layers are not supported by the plugin for specified device {}:\\n {}\".\n",
    "                      format(plugin.device, ', '.join(not_supported_layers)))\n",
    "            print(\"Please try to specify cpu extensions library path in sample's command line parameters \"\n",
    "                  \"using -l or --cpu_extension command line argument\")\n",
    "            return None\n",
    "    return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**:\n",
    "Some models may be incompatible with some target devices. For example, some types of neural network layers are not supported on the CPU target. \n",
    "\n",
    "### 1.4 Load the network into the plugin\n",
    "\n",
    "Once we have the plugin and the network, we can load the network into the plugin using **`plugin.load`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadNetwork(plugin, net):\n",
    "    # Loading IR model to the plugin.\n",
    "    exec_net = plugin.load(network=net, num_requests=2)\n",
    "    \n",
    "    # Getting the input and outputs of the network\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    return exec_net,input_blob,out_blob\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Run inference\n",
    "\n",
    "Now we are ready to try running the inference workload using the plugin.\n",
    "First let's load the image using OpenCV.\n",
    "We will also have to do some shape manipulation to convert the image to a format that is compatible with our network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def preprocessImage(img_path, net, input_blob):\n",
    "    # Reading the frame from a jpeg file\n",
    "    frame = cv2.imread(img_path)\n",
    "    \n",
    "    # Reshaping data\n",
    "    n, c, h, w = net.inputs[input_blob].shape\n",
    "    in_frame = cv2.resize(frame, (w, h))\n",
    "    in_frame = in_frame.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "    return in_frame.reshape((n, c, h, w)),frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference, we will be running in **async_mode** by using `start_async` method. \n",
    "With the async_mode, the inference is started in parallel on either a separate thread or device.\n",
    "In other words, `start_async` is non-blocking and the main process is free to do any additional processing needed. \n",
    "In the next section, we will see an implementation of pipelining to mask the latency of loading and modifying images.\n",
    "\n",
    "During asynchronous runs, the different images are tracked by an integer `request_id`. \n",
    "Because we only have one image to process, we will just use 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For labeling the image\n",
    "from out_process import placeBoxes\n",
    "\n",
    "\n",
    "# Request id to keep track of\n",
    "def runInference():\n",
    "    plugin = createPlugin(device='CPU', extension_list=['/data/reference-sample-data/extension/libcpu_extension.so'])\n",
    "    model_xml = \"models/mobilenet-ssd/FP32/mobilenet-ssd.xml\"\n",
    "    model_bin = \"models/mobilenet-ssd/FP32/mobilenet-ssd.bin\"\n",
    "    net = createNetwork(model_xml, model_bin, plugin)\n",
    "    exec_net,input_blob,out_blob = loadNetwork(plugin, net)\n",
    "    in_frame,original_frame = preprocessImage('cars_1900_first_frame.jpg', net, input_blob)\n",
    "    \n",
    "    my_request_id=0\n",
    "\n",
    "    # Starting the inference in async mode, which starts the inference in parallel\n",
    "    exec_net.start_async(request_id=my_request_id, inputs={input_blob: in_frame})\n",
    "    # ... You can do additional processing or latency masking while we wait ...\n",
    "\n",
    "    # Blocking wait for a particular request_id\n",
    "    if exec_net.requests[my_request_id].wait(-1) == 0:\n",
    "        # getting the result of the network\n",
    "        res = exec_net.requests[my_request_id].outputs[out_blob]\n",
    "\n",
    "        # Processing the output result and adding labels on the image. Implementation is not shown in the\n",
    "        #  this notebook; you can find it in object_detection_demo_ssd_async.py\n",
    "        prob_threshold = 0.5  # 50% confidence needed for \"detection\"\n",
    "        initial_w = original_frame.shape[1]\n",
    "        initial_h = original_frame.shape[0]\n",
    "        frame = placeBoxes(res, None, prob_threshold, original_frame, initial_w, initial_h, False, my_request_id, 0)\n",
    "        # We use pyplot because it plays nicer with Jupyter Notebooks\n",
    "        fig = plt.figure(dpi=300)\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB), interpolation='none')\n",
    "        plt.axis(\"off\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"There was an error with the request\")\n",
    "runInference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Inference on a video\n",
    "\n",
    "Now that we know how to run inference on a single frame, let's extend this to multiple frames.\n",
    "This part is already implemented in \n",
    "<a href=\"object_detection_demo_ssd_async.py\">object_detection_demo_ssd_async.py</a>.\n",
    "Most of the code is just an extension of the single frame example, but there are few points of importance that we highlight here.\n",
    "\n",
    "The following lines determine the source of the video. We will use a pre-recorded input video file in this example, we could also use a camera by setting the input argument to 'cam'.\n",
    "```python\n",
    "if args.input == 'cam':\n",
    "        input_stream = 0\n",
    "        out_file_name = 'cam'\n",
    "    else:\n",
    "        input_stream = args.input\n",
    "```\n",
    "We capture frames from the video sample using **OpenCV VideoCapture** API.\n",
    "\n",
    "```python\n",
    "cap = cv2.VideoCapture(input_stream)\n",
    "```\n",
    "\n",
    "Finally, we have a latency masking scheme, where we post-process a frames while another frame is being processed on the inference engine.\n",
    "\n",
    "```python\n",
    "cur_request_id = 0\n",
    "next_request_id = 1\n",
    "\n",
    "while cap.isOpened():\n",
    "   # ... load next frame from cap ...\n",
    "\n",
    "   # start the next frame\n",
    "   exec_net.start_async(request_id=next_request_id, inputs={input_blob: in_frame})\n",
    "\n",
    "   # see if the current frame is ready\n",
    "   if exec_net.requests[cur_request_id].wait(-1) == 0:\n",
    "       # ... post-processing current frame ...\n",
    "    \n",
    "   # swap request ids\n",
    "   cur_request_id, next_request_id = next_request_id, cur_request_id\n",
    "```\n",
    "\n",
    "The python code takes in command line arguments for video, model etc.\n",
    "\n",
    "**Command line arguments options and how they are interpreted in the application source code**\n",
    "\n",
    "```\n",
    "SAMPLEPATH=\"/data/reference-sample-data\"\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i ${SAMPLEPATH}/object-detection-python/cars_1900.mp4 \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l ${SAMPLEPATH}/extension/libcpu_extension.so\n",
    "```\n",
    "\n",
    "##### The description of the arguments used in the argument parser is the command line executable equivalent.\n",
    "* -m location of the **mobilenet-ssd** pre-trained model which has been pre-processed using the **model optimzer**\n",
    "   There is automated support built in this argument to support both FP32 and FP16 models targeting different hardware\n",
    "   (**Note** we are using mobilenet-ssd in this example. However, OpenVINO's Inference Engine is compatible with other neural network architectures such as AlexNet*, GoogleNet*, MxNet* etc.,)    \n",
    "\n",
    "* -i location of the input video stream (video/cars_1900.mp4)\n",
    "* -o location where the output file with inference needs to be stored. (results/core or results/xeon or results/gpu)\n",
    "* -d Type of Hardware Acceleration (CPU or GPU or MYRIAD)\n",
    "* -l Absolute path to the shared library and is currently optimized for core/xeon (extension/libcpu_extension.so )\n",
    "\n",
    "### 2.1 Creating job file\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel Xeon Scalable processor, where the Notebook is allocated a single core. \n",
    "To run inference on the entire video, we need more compute power.\n",
    "We will run the workload on several edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job file for you in the notebook.\n",
    "Run the following cell to write this in to the file \"object_detection_job.sh\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile object_detection_job.sh\n",
    "\n",
    "# The default path for the job is your home directory, so we change directory to where the files are.\n",
    "cd $PBS_O_WORKDIR\n",
    "\n",
    "# Object detection script writes output to a file inside a directory. We make sure that this directory exists.\n",
    "# The output directory is the first argument of the bash script\n",
    "\n",
    "if [ \"$2\" = \"HETERO:FPGA,CPU\" ]; then\n",
    "    # Environment variables and compilation for edge compute nodes with FPGAs\n",
    "    source /opt/intel/computer_vision_sdk/bin/setup_hddl.sh\n",
    "    aocl program acl0 /opt/intel/computer_vision_sdk_2018.4.420/bitstreams/a10_vision_design_bitstreams/4-0_PL1_FP11_MobileNet_ResNet_VGG_Clamp.aocx\n",
    "fi\n",
    "    \n",
    "# Running the object detection code\n",
    "SAMPLEPATH=$PBS_O_WORKDIR\n",
    "python3 object_detection_demo_ssd_async.py -m ${SAMPLEPATH}/models/mobilenet-ssd/$3/mobilenet-ssd.xml \\\n",
    "                                           -i /data/reference-sample-data/object-detection-python/cars_1900.mp4 \\\n",
    "                                           -o $1 \\\n",
    "                                           -d $2 \\\n",
    "                                           -l /data/reference-sample-data/extension/libcpu_extension.so\n",
    "\n",
    "g++ -std=c++14 ROI_writer.cpp -o ROI_writer  -lopencv_core -lopencv_videoio -lopencv_imgproc -lopencv_highgui  -fopenmp -I/opt/intel/computer_vision_sdk/opencv/include/ -L/opt/intel/computer_vision_sdk/opencv/lib/\n",
    "\n",
    "# Rendering the output video\n",
    "SKIPFRAME=1\n",
    "RESOLUTION=0.5\n",
    "./ROI_writer \"/data/reference-sample-data/object-detection-python/cars_1900.mp4\" $1 $SKIPFRAME $RESOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup the output video specs:\n",
    "\n",
    "\n",
    "After running the object detection code, we have a list of bounding boxes for the detected objects. This may be the end of the object detection workflow some applications that just need the locations, sizes and types of the detected objects. However, if a human needs to view the output video stream, we may want to produce an output video where the bounding boxes are drawn on top of the detected objects.\n",
    "\n",
    "We treat video rendering as a separate task, which is invoked by ROI_writer at the end of our job. If you don't want to spend a long time rendering the video, you can reduce the output video quality usinng the SKIP_FRAME and RESOLUTION variables:\n",
    "\n",
    "* SKIP_FRAME=1 will write all processed video frames with bounding boxes into the output video (this is the slowest option and it preserves all inference data in the output video stream)\n",
    "* SKIP_FRAME>1 will write only some of the processed frames into the output video (e.g., SKIP_FRAME=2 writes every other frame, SKIP_FRAME=10 writes one frame out of 10)\n",
    "* RESOLUTION=1 will produce the output video with the same resolution as the input video (this is the slowest option)\n",
    "* RESOLUTION<1 will reduce the output video desolution (e.g., RESOLUTION=0.5 will set the output video resolution in each dimension to 50% of the input video)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit object_detection_job to 5 different types of edge compute nodes simultaneously or just one node at at time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them.\n",
    "\n",
    "If you are curious to see the available types of nodes on the IoT DevCloud, run the following optional cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pbsnodes | grep properties | sort | uniq -c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the properties describe the node, and number on the left is the number of available nodes of that architecture.\n",
    "\n",
    "### 2.3 Job queue submission\n",
    "\n",
    "Each of the 4 cells below will submit a job to different edge compute nodes.\n",
    "The output of the cell is the `JobID` of your job, which you can use to track progress of a job.\n",
    "\n",
    "**Note** You can submit all 4 jobs at once or follow one at a time. \n",
    "\n",
    "After submission, they will go into a queue and run as soon as the requested compute resources become available. \n",
    "(tip: **shift+enter** will run the cell and automatically move you to the next cell. So you can hit **shift+enter** multiple times to quickly run multiple cells)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_core = !qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/core CPU FP32\" -N obj_det_core\n",
    "print(job_id_core[0]) \n",
    "#Progress indicators\n",
    "if job_id_core:\n",
    "    progressIndicator('results/core', 'i_progress_'+job_id_core[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/core', 'v_progress_'+job_id_core[0]+'.txt', \"Rendering\", 0, 100)\n",
    "else:\n",
    "    print(\"Error in job submission.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Xeon CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Xeon CPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_xeon = !qsub object_detection_job.sh -l nodes=1:iei-tank-xeon -F \"results/xeon CPU FP32\" -N obj_det_xeon \n",
    "print(job_id_xeon[0]) \n",
    "#Progress indicators\n",
    "if job_id_xeon:\n",
    "    progressIndicator('results/xeon', 'i_progress_'+job_id_xeon[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/xeon', 'v_progress_'+job_id_xeon[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Core CPU and using the onboard Intel GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Core CPU and an Intel GPU...\")\n",
    "#Submit job to the queue\n",
    "job_id_gpu = !qsub object_detection_job.sh -l nodes=1:iei-tank-core -F \"results/gpu GPU FP32\" -N obj_det_gpu \n",
    "print(job_id_gpu[0]) \n",
    "#Progress indicators\n",
    "if job_id_gpu:\n",
    "    progressIndicator('results/gpu', 'i_progress_'+job_id_gpu[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/gpu', 'v_progress_'+job_id_gpu[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel Movidius NCS (Neural Computing Stick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel Movidius NCS...\")\n",
    "#Submit job to the queue\n",
    "job_id_myriad = !qsub object_detection_job.sh -l nodes=1:iei-tank-movidius -F \"results/myriad MYRIAD FP16\" -N obj_det_myriad\n",
    "print(job_id_myriad[0]) \n",
    "#Progress indicators\n",
    "if job_id_myriad:\n",
    "    progressIndicator('results/myriad', 'i_progress_'+job_id_myriad[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/myriad', 'v_progress_'+job_id_myriad[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### submitting to a node with Intel FPGA HDDL-F (High Density Deep Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Submitting a job to an edge compute node with an Intel FPGA HDDL-F...\")\n",
    "#Submit job to the queue\n",
    "job_id_fpga = !qsub object_detection_job.sh -l nodes=1:iei-tank-fpga -F \"results/fpga HETERO:FPGA,CPU FP32\" -N obj_det_fpga\n",
    "print(job_id_fpga[0]) \n",
    "#Progress indicators\n",
    "if job_id_fpga:\n",
    "    progressIndicator('results/fpga', 'i_progress_'+job_id_fpga[0]+'.txt', \"Inference\", 0, 100)\n",
    "    progressIndicator('results/fpga', 'v_progress_'+job_id_fpga[0]+'.txt', \"Rendering\", 0, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job in step 2.3).\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wait!***\n",
    "\n",
    "Please wait for the inference jobs and video rendering complete before proceeding to the next step.\n",
    "\n",
    "## Step 3: View Results\n",
    "\n",
    "Once the jobs are completed, the queue system outputs the stdout and stderr streams of each job into files with names of the form\n",
    "\n",
    "`obj_det_{type}.o{JobID}`\n",
    "\n",
    "`obj_det_{type}.e{JobID}`\n",
    "\n",
    "(here, obj_det_{type} corresponds to the `-N` option of qsub).\n",
    "\n",
    "However, for this case, we may be more interested in the output video files. They are stored in mp4 format inside the `results/` directory.\n",
    "We wrote a short utility script that will display these videos with in the notebook.\n",
    "Run the cells below to display them.\n",
    "See `demoutils.py` if you are interested in understanding further how the results are displayed in notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank (Intel Core CPU)', ['results/core/cars_1900_output_'+job_id_core[0]+'.mp4'], 'results/core/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank Xeon (Intel Xeon CPU)',['results/xeon/cars_1900_output_'+job_id_xeon[0]+'.mp4'],'results/xeon/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Intel GPU (Intel Core + Onboard GPU)', ['results/gpu/cars_1900_output_'+job_id_gpu[0]+'.mp4'],'results/gpu/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + Intel FPGA HDDL-F',['results/fpga/cars_1900_output_'+job_id_fpga[0]+'.mp4'],'results/fpga/stats.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videoHTML('IEI Tank + (Intel CPU + Movidius)',['results/myriad/cars_1900_output_'+job_id_myriad[0]+'.mp4'],'results/myriad/stats.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Assess Performance\n",
    "\n",
    "The running time of each inference task is recorded in `results/*/stats.txt`, where the subdirectory name corresponds to the architecture of the target edge compute node. Run the cell below to plot the results of all jobs side-by-side. Lower values mean better performance. Keep in mind that some architectures are optimized for the highest performance, others for low power or other metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaryPlot({'results/core/stats.txt':'Core', 'results/xeon/stats.txt':'Xeon', 'results/gpu/stats.txt':'GPU', 'results/fpga/stats.txt':'FPGA', 'results/myriad/stats.txt':'Myriad'}, 'Architecture', 'Time, seconds', 'Inference engine processing time' )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
